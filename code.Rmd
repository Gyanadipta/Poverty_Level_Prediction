---
title: "Poverty Level Analysis"
output: html_document
---

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

### Loading Libraries

```{r setup, echo=FALSE}
library(tidyverse) # importing, cleaning, visualising
library(xgboost) # gradient boosted machines
library(caret) # ml pipelines made easy
library(VIM) # missing data
library(corrplot) # correlation visualisation
library(ggthemes) # extra plotting themes
library(plotly) # interactive plots
library(factoextra) # pca visualisation
library(Rtsne) # visualisation of high dimensional datsets
library(breakDown) # visualising model results
library(gridExtra) # arranging plots
library(lightgbm) # leaf-wise gbm
library(glmnet) # regularised regression

# Disable scientific notation
options(scipen = 999)
```

### Importing Data 

```{r}
train = read_csv("./Data/train.csv")
test = read_csv("./Data/test.csv")

cat("Dimensions for training data : ", dim(train))
cat("Dimensions for testing data: ", dim(test))

head(train)
```

Encoding categorical features (Already one-hot encoded, reversing present encoding, adding new integer encoding)

One hot encoding is one method of converting data to prepare it for an algorithm and get a better prediction. With one-hot, we convert each categorical value into a new categorical column and assign a binary value of 1 or 0 to those columns. Each integer value is represented as a binary vector.

```{r, echo=FALSE}
full = as_tibble(rbind(train %>% select(-Target), test))

feature_list = c(
  "pared",
  "piso",
  "techo",
  "abasta",
  "sanitario",
  "energcocinar",
  "elimbasu",
  "epared",
  "etecho",
  "eviv",
  "estadocivil",
  "parentesco",
  "instlevel",
  "tipovivi",
  "lugar",
  "area"
)

new_features_integer = data.frame(matrix(ncol = length(feature_list), nrow = nrow(full)))
```


Cycle through and reverse the OHE process for these

```{r, echo=FALSE}

ohe_names = vector()

for(i in 1:length(feature_list)){
  
  # Grab the feature
  
  feature_to_fix = full %>% select(starts_with(feature_list[i]))
  
  # Fix and enter into our new feature matrix
  
  new_features_integer[,i] = as.integer(factor(names(feature_to_fix)[max.col(feature_to_fix)], ordered = FALSE))
  names(new_features_integer)[i] = paste0(feature_list[i],"_int")
  
  ohe_names = c(ohe_names, as.vector(names(feature_to_fix)))
  
}

head(new_features_integer)

# Resultant dataset 
full = data.frame(cbind(full,new_features_integer), stringsAsFactors = FALSE)

head(full)
```

### Introducing Ridge Encoding

```{r}
# Grab OHE features

new_features_integer = data.frame(apply(new_features_integer,2,factor))
ridge_features = model.matrix(~.-1,new_features_integer[1:nrow(train),])
ridge_response = ifelse(train$Target == 4, 0, 1)


# Run ridge

cv.fit = cv.glmnet(
    x = as.matrix(ridge_features),
    y = as.factor(ridge_response),
    nfolds = 5,
    alpha = 0,
    family = "binomial"
)

plot(cv.fit)
```
### Final preprocessing

```{r}
# Rather than taking the predictions themselves, we'll take the coeffecients as encoding

tmp_coeffs = coef(cv.fit, s = "lambda.min")
coefs = data.frame(name = tmp_coeffs@Dimnames[[1]][tmp_coeffs@i + 1], coefficient = tmp_coeffs@x)


# Lookup

# Adjust original feature level names

for (i in 1:ncol(new_features_integer)){
  new_features_integer[,i] = paste0(names(new_features_integer)[i], new_features_integer[,i])
}

new_features_ridge <- new_features_integer
new_features_ridge[] <- coefs$coefficient[match(unlist(new_features_integer), coefs$name)]

# Zeros can replace the NA since these are just base level

new_features_ridge[is.na(new_features_ridge)] = 0

# Rename for now ...

names(new_features_ridge) = gsub("int","ridge",names(new_features_ridge))

# Attach to data

full = data.frame(cbind(full,new_features_ridge), stringsAsFactors = FALSE)
```

### Visualization

```{r}
# Filter the data for those features that actually have some missing data

full_missing = full[, sapply(full, anyNA), drop = FALSE]

cat("Missing data found in ", ncol(full_missing)/ncol(full)*100, "% of features")

# Plot missing information for these features

aggr(full_missing, prop = T, numbers = T, cex.axis = 0.8)
```
### Correlation

```{r}
# Select numeric features (removing the categorical features for now and those with high % NA)

full_numerical = full %>% 
  select_if(is.numeric) %>% 
  select(-one_of(ohe_names)) %>% 
  select(
    -v2a1,
    -v18q1,
    -rez_esc
  )

# Since the number of NA is so low for the remaining features I will just replace them with their column mean

replace_na = function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
full_numerical = apply(full_numerical, 2, replace_na)

# Create the matrix

full_numerical_cor = cor(full_numerical, use = "complete.obs")

# Plot the matrix with some clustering

corrplot(full_numerical_cor, order = "hclust", tl.cex = 0.5)
```
```{r}
# We have already created a numerical matrix with the categorical information excluded, so we will use this

full_pca = prcomp(full_numerical, center = TRUE, scale. = TRUE)

full = full %>% 
  cbind(full_pca$x[,1:3])

# scree plot

fviz_eig(full_pca)
```

```{r}
# plot first two components
fviz_pca_var(full_pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             select.var = list(contrib = 30), # top 30 contributing
             repel = TRUE     # Avoid text overlapping
             )
```

```{r}
# Grab training set for visualisation

train_pca = full[1:nrow(train),] %>% 
  select(PC1,PC2,PC3) %>% 
  cbind(train$Target) %>% 
  rename(Target = `train$Target`) %>% 
  mutate(
    Target = as.factor(Target),
    Target_Binary = ifelse(Target == 4, "Non-vunerable", "Vunerable"))

# Plot

plot_ly(train_pca, x = ~PC1, y = ~PC2, z = ~PC3, color = ~Target) %>%
  add_markers(marker = list(
      size = 3,
      opacity = 0.5)) %>%
  layout(scene = list(xaxis = list(title = 'PC1'),
                     yaxis = list(title = 'PC2'),
                     zaxis = list(title = 'PC3'),
         title = 'PCA plot (original target)'))
```

```{r}
plot_ly(train_pca, x = ~PC1, y = ~PC2, z = ~PC3, color = ~Target_Binary) %>%
  add_markers(marker = list(
      size = 3,
      opacity = 0.5)) %>%
  layout(scene = list(xaxis = list(title = 'PC1'),
                     yaxis = list(title = 'PC2'),
                     zaxis = list(title = 'PC3'),
         title = 'PCA plot (binary target)'))
```

```{r}
# Run tsne on sample of numerical features we used for PCA (training set only)

train_tsne = full_numerical[1:nrow(train),] 

# Appears we have duplicates in our data - let's look

train_duplicates = which(duplicated(train_tsne))

# 23 instances: I suppose this could happen since we may have two adults in the same house with all numerical features being identical

# Remove dupes and scale

train_tsne_deduped = train_tsne[-train_duplicates,]
train_tsne_labels = train[-train_duplicates, "Target"]
train_tsne_labels = as.factor(train_tsne_labels$Target)


# Let's also include a binary version of our target to make visualisation easier

train_tsne_labels_binary = train[-train_duplicates, "Target"] %>% 
  mutate(
    Target_Binary = ifelse(Target == 4, "Non-vunerable", "Vunerable")
  )
train_tsne_labels_binary = as.factor(train_tsne_labels_binary$Target_Binary)

tsne = Rtsne(
  train_tsne_deduped, 
  dims = 3, 
  perplexity = 50, 
  max_iter = 500, 
  verbose = TRUE, 
  theta = 0.35,
  pca_scale = TRUE)
```

```{r}
# Plot with binary labels 

tsne_plot_binary <- data.frame(x = tsne$Y[,1], y = tsne$Y[,2], z = tsne$Y[,3], Target = train_tsne_labels_binary)

plot_ly(tsne_plot_binary, x =~x, y =~y, z =~z, color = ~Target) %>%
  add_markers(marker = list(
      size = 3,
      opacity = 0.5)) %>%
  layout(scene = list(xaxis = list(title = 'D1'),
                     yaxis = list(title = 'D2'),
                     zaxis = list(title = 'D3')),
         title = 't-SNE plot (binary target)')
```

```{r}
# Select features

important_features = c(
  "escolari",
  "dependency",
  "age",
  "qmobilephone",
  "PC1",
  "PC2"
)

important = full[1:nrow(train),] %>% cbind(train$Target) %>%
  rename(Target = `train$Target`) %>% 
  select(one_of(important_features), Target) %>% 
  mutate(    
    dependency = as.numeric(ifelse(dependency == 'no', 0, 
                                   ifelse(dependency == 'yes', mean(dependency, na.rm = TRUE), dependency)))
    )

featurePlot(x=important[,1:6], y=as.factor(important[,7]),            
            plot = "box", 
            scales = list(y = list(relation="free"),
                          x = list(rot = 90)),  
            layout = c(3, 2), 
            auto.key = list(columns = 3))
```

```{r}
featurePlot(x=important[,1:6], y=as.factor(important[,7]),            
            plot = "density", 
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")), 
            adjust = 1.5, 
            pch = "|", 
            layout = c(3, 2), 
            auto.key = list(columns = 3))
```

```{r}
# Add split label

train_test_data = full %>% select(one_of(important_features)) %>% 
    mutate(    
    dependency = as.numeric(ifelse(dependency == 'no', 0, 
                                   ifelse(dependency == 'yes', mean(dependency, na.rm = TRUE), dependency)))
    )
train_test_data[1:nrow(train),"Split"] = "Train"
train_test_data[(nrow(train)+1):nrow(train_test_data),"Split"] = "Test"


# Have a look at the different distributions


featurePlot(x=train_test_data[,important_features], y=as.factor(train_test_data[,"Split"]),            
            plot = "density", 
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")), 
            adjust = 1.5, 
            pch = "|", 
            layout = c(3, 2), 
            auto.key = list(columns = 3))
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
